[Zpět na úvodní stránku](../README.md)

### Motivace
Při učení s lidským užitelem při hraní šachů bude obtížné aby stroj byl lepší než člověk - Můžeme ale poskytovat informaci, jak moc byl dobrý a dávat zpětnou vazbu

##### Uvažujeme model robota, který se učí chodit
- Při učení s učitelem je potřeba mu ukázat, jak vypadá strandardní chůze a ukátaz mu spoustu různých možností
- Při učení bez učitele - může se nalétz různé vzorce chůze, ale bez zpětné vazby nemá žádné materiály pro rozhodnutí

# Zpětnovazebné učení (reinforcmetn learning)
Učení umělé inteligence pomocí zpětné vazby
- Agent je umístěn do prostředí a musí se v něm naučit úspěšně chovat

## typy RL
- Model-based: Kompletní informace o prostředí, lze plínovat dopředu
- Model-free: na základě interakce získá zpětnou vazbu, nezle předpovídat

## Typy agentů
- se zádostmi - se učí funkci užitlu pro stavy
- s Q-učením - učí se akce na základě očekávaného užitku
- Reflexní agent - učí se strategii, jakou akci použít na daný stav
- Pasivní učení - pevná strategie a učí se užitek stavů/akcí
- A ktivní učení - porozkoumávání prostředí

## Pojmy ve zpětnovazebném učení
**Agent** - dělá akce, pohybuje se v prostředí, dostává odměny

**Akce (action)** - množina všech možných operací

**Prostředí (enviroment)** - prostředí ve kterém se agent pohybuje

**Stav (state - S) - situace ve které se agent nacházi**

**Odměna (reward – R)**  - prostředí poskytuje odměnu, definující úspěšnost agenta

**Diskontní faktor - (discount factor)** - snižuje budoucí odměny

**Strategie (policy π)** - rozhoduje, jakou akci podniknout v určitém stavu za účelem maximalizace odměny.

**Hodnota (value)** - kvantifikuje optimální nebo specifickou hodnotu stavu. Tzn. Je to očekávaná slevněná odměna.

**Q-hodnota nebo hodnota acke - (Q-value)** - hodnota je míra celkové očekávané odměny, pokud je agent (A) ve stavu (s) a provede akci (a), a poté hraje do konce epizody podle nějaké strategie (π).

## Složeníé modelu zpětnovazebního učení
Celý proces spočívá v tom, že agent se snaží optimalizovat své akce na základě zpětné vazby od prostředí tak, aby maximalizoval svou celkovou odměnu. Tento cyklus se opakuje a agent se postupně učí lepší strategie pro interakci s prostředím.

## Markovovy řetězce
Markovovy řetězce jsou modely náhodných procesů, ve kterých přechody mezi stavy jsou popsány pravděpodobnostmi a závisejí pouze na aktuálním stavu (ne na předchozích stavech). Tento princip se nazývá Markovova vlastnost.

### Markovův rozhodovací proces
Cílem Markovova rozhodovacího procesu je najít optimální politiku rozhodování, tj. strategii, která maximalizuje očekávanou kumulativní odměnu v průběhu času.

## Bellmanova rovnice
Tato rovnice v podstatě vyjadřuje hodnotu stavu jako součet okamžitých odměn a slevy z hodnot budoucích stavů. Používá se k formulaci optimálních strategií v problémech rozhodování s nejistotou a sekvencemi rozhodnutí v čase.

## Pasivní učení
Pevně daná strategie  
Učení je založené na užitku z dosahovaných stavů vykonaných akcí  
Agent nezná přechodový model a ani užitkovou funkci - snaží se procházet prostředí za použití dané strategie

### Přímý model
Pro stavy v každém vzorku vypočte ocenění stavů cesty od počátku do cíle  
Protože jeden stav se může vyskytovat ve více vzorcích ohodnotíme stav tak, že uděláme průměr ocenění cest  
**V podstatě se jedná o učení s učitelem**

### Další metody pasívního učení
#### Adaptivní dynamické programování
Adaptivní dynamické programování je přístup k řešení problémů rozhodování, který se odlišuje od přímého modelu tím, že zahrnuje Bellmanovy rovnice. Užitek se vypočítává pomocí iterací hodnot nebo strategií.

#### Temporální diferenceje 
je technika, která upravuje hodnoty stavů přímo, místo aby se učila přechodové tabulky

**Celkově lze říci**, že adaptivní dynamické programování a temporální diference přinášejí metody pro řešení problémů rozhodování bez potřeby kompletního přehledu o přechodech mezi stavy, což může být v praxi užitečné.


## Aktivní učení
- pasivní učení je předem dána strategie určující chování agenta.
- U aktivního učení se agent učí (učí se strategii) co má dělat a zkoumá k tomu prostředí. 
- Aktivní agent se sám rozhoduje, jaké akce provede (a tím určuje i strategii).

### Hladový agent
Hladový agent se rozhoduje na základě okamžitých informací, které má k dispozici, a volí akce, které vedou k okamžitému zisku nebo výhodě. Agent neplánuje dopředu a nezohledňuje možné budoucí následky svých rozhodnutí. Jeho cílem je maximalizovat okamžitý prospěch bez ohledu na to, zda to v dlouhodobém horizontu přinese optimální výsledek.

### Q-učení
Jednoduše řečeno, Q-učení se snaží naučit funkci Q(s, a), která představuje odměnu, kterou agent může očekávat, když se nachází ve stavu s a provede akci a.  
Algoritmus se postupně učí optimalizovat tuto Q-funkci tak, aby agent dokázal rozhodnout, jaké akce přijmout v závislosti na aktuálním stavu, aby maximalizoval svou kumulativní odměnu v čase.  
Tento algoritmus se zaměřuje na učení odměn (Q-hodnot) pro páry stav-akce a je často využíván v problémech, kde agent musí rozhodovat, jaké akce podniknout ve stavu prostředí.




